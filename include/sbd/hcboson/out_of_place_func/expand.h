/**
@file sbd/hcboson/out_of_place_func/expand.h
@brief A set of functions to determine the expanded space generated by applying the Hamiltonian operator
*/
#ifndef SBD_HCBOSON_OUT_OF_PLACE_FUNC_EXPAND_H
#define SBD_HCBOSON_OUT_OF_PLACE_FUNC_EXPAND_H

namespace sbd {

  template <typename ElemT>
  void MeasHamSquare(const GeneralOp<ElemT> & H,
		     const Basis & B,
		     const std::vector<ElemT> & W,
		     size_t bit_length,
		     MPI_Comm & h_comm,
		     MPI_Comm & comm,
		     ElemT & res) {

    int mpi_h_master = 0;
    int mpi_h_rank; MPI_Comm_rank(h_comm,&mpi_h_rank);
    int mpi_h_size; MPI_Comm_size(h_comm,&mpi_h_size);

    int mpi_master = 0;
    int mpi_size; MPI_Comm_size(comm,&mpi_size);
    int mpi_rank; MPI_Comm_rank(comm,&mpi_rank);

    // We first generate all possible space. Then, perform sorting.
    std::vector<size_t> config_0 = B.Config(static_cast<size_t>(0));
    size_t c_len = config_0.size();
    size_t num_b = B.Size();
    size_t num_h = H.NumOpTerms();
    std::vector<std::vector<size_t>> config;
    std::vector<ElemT> weight;

    if( mpi_h_rank == mpi_h_master ) {
      config.reserve(num_b*(num_h+1));
      weight.reserve(num_b*(num_h+1));
    } else {
      config.reserve(num_b*num_h);
      weight.reserve(num_b*num_h);
    }

    size_t nc = 0;
    if( mpi_h_rank == mpi_h_master ) {
      config.resize(num_b);
      weight.resize(num_b);
      std::vector<std::vector<size_t>> B_config = B.Config();
      std::copy(B_config.begin(),B_config.end(),config.begin());
    // diagonal terms
#pragma omp parallel
      {
	std::vector<size_t> v;
	size_t size_t_one = 1;
	bool check;
	ElemT val;
#pragma omp for
	for(size_t is=0; is < num_b; is++) {
	  v = B.Config(is);
	  val = ElemT(0.0);
	  for(size_t n=0; n < H.d_.size(); n++) {
	    check = false;
	    for(int k=0; k < H.d_[n].n_dag_; k++) {
	      size_t q = static_cast<size_t>(H.d_[n].fops_[k].q_);
	      size_t r = q / bit_length;
	      size_t x = q % bit_length;
	      if( ( v[r] & ( size_t_one << x ) ) == 0 ) {
		check = true;
		break;
	      }
	    }
	    if( check ) continue;
	    val += H.e_[n] * W[is];
	  }
	  weight[is] = val;
	}
      } // end omp parallel scope
    } // end if ( mpi_h_rank == mpi_h_master )

    
    // generate all configurations
#pragma omp parallel
    {
      std::vector<std::vector<size_t>> local_config;
      std::vector<ElemT> local_weight;
      
      std::vector<size_t> v;
      std::vector<size_t> w;
      size_t size_t_one = 1;
      bool check;
#pragma omp for
      for(size_t is=0; is < num_b; is++) {
	v = B.Config(is);
	for(size_t n=0; n < H.o_.size(); n++) {
	  w = v;
	  check = false;
	  for(int k=0; k < H.o_[n].n_dag_; k++) {
	    size_t q = static_cast<size_t>(H.o_[n].fops_[k].q_);
	    size_t r = q / bit_length;
	    size_t x = q % bit_length;
	    if( ( w[r] & ( size_t_one << x ) ) != 0 ) {
	      w[r] = w[r] ^ ( size_t_one << x );
	    } else {
	      check = true;
	      break;
	    }
	  }
	  if ( check ) continue;
	  for(int k = H.o_[n].n_dag_; k < H.o_[n].fops_.size(); k++) {
	    size_t q = static_cast<size_t>(H.o_[n].fops_[k].q_);
	    size_t r = q / bit_length;
	    size_t x = q % bit_length;
	    if( ( w[r] & ( size_t_one << x ) ) == 0 ) {
	      w[r] = w[r] | ( size_t_one << x );
	    } else {
	      check = true;
	      break;
	    }
	  }
	  if ( check ) continue;
	  
	  // now w is the configuration after the Hamiltonian operation
	  local_config.push_back(w);
	  local_weight.push_back(H.c_[n] * W[is]);
	} // end loop for generating new configurations
      } // end omp parallel for
#pragma omp critical
      {
	// config.push_back(std::move(local_config));
	// weight.push_back(std::move(local_weight));
	config.insert(config.end(),
		      std::make_move_iterator(local_config.begin()),
		      std::make_move_iterator(local_config.end()));
	weight.insert(weight.end(),
		      std::make_move_iterator(local_weight.begin()),
		      std::make_move_iterator(local_weight.end()));
      }
    } // end omp parallel scope
    nc = config.size();
    
    // perform full sort for all process
    std::vector<std::vector<size_t>> new_config(config);
    std::vector<std::vector<size_t>> config_begin(mpi_size,std::vector<size_t>(c_len));
    std::vector<std::vector<size_t>> config_end(mpi_size,std::vector<size_t>(c_len));
    std::vector<size_t> index_begin(mpi_size);
    std::vector<size_t> index_end(mpi_size);
    mpi_sort_bitarray(new_config,config_begin,config_end,index_begin,index_end,bit_length,comm);

    // determine the node to be send
#ifdef SBD_DEBUG
    for(int rank=0; rank < mpi_size; rank++) {
      if( mpi_rank == rank ) {
	std::cout << " Start send data preparation: ";
	std::cout << " Sizes: config.size() = " << config.size();
	std::cout << " while nc = " << nc;
	std::cout << " config_begin.size() = " << config_begin.size() << std::endl;
      }
      MPI_Barrier(comm);
    }
#endif

    std::vector<std::vector<std::vector<size_t>>> config_send(mpi_size);
    std::vector<std::vector<ElemT>> weight_send(mpi_size);

    int target_mpi;
    bool mpi_exist;
    for(size_t n=0; n < nc; n++) {
      mpi_process_search(config[n],config_begin,config_end,target_mpi,mpi_exist);
      config_send[target_mpi].push_back(config[n]);
      weight_send[target_mpi].push_back(weight[n]);
    }

#ifdef SBD_DEBUG
    for(int rank=0; rank < mpi_size; rank++) {
      if( mpi_rank == rank ) {
	std::cout << " End send data prepration:";
	for(int target=0; target < mpi_size; target++) {
	  std::cout << " config_send[" << target << "] = " << config_send[target].size();
	}
	std::cout << std::endl;
      }
      MPI_Barrier(comm);
      sleep(5);
    }
#endif
    
    std::vector<std::vector<std::vector<size_t>>> config_recv(mpi_size);
    std::vector<std::vector<ElemT>> weight_recv(mpi_size);

    config_recv[mpi_rank] = config_send[mpi_rank];
    weight_recv[mpi_rank] = weight_send[mpi_rank];

    for(size_t slide=1; slide < mpi_size; slide++) {
      int mpi_send_rank = (mpi_size+mpi_rank+slide) % mpi_size;
      int mpi_recv_rank = (mpi_size+mpi_rank-slide) % mpi_size;
      MpiSlide(config_send[mpi_send_rank],config_recv[mpi_recv_rank],slide,comm);
      MpiSlide(weight_send[mpi_send_rank],weight_recv[mpi_recv_rank],slide,comm);
    }

    std::vector<ElemT> new_weight(new_config.size(),ElemT(0.0));
    size_t idx_begin_this = 0;
    size_t idx_end_this = new_config.size();
    bool exist;
    size_t idx;
    for(size_t rank=0; rank < mpi_size; rank++) {
      for(size_t n=0; n < config_recv[rank].size(); n++) {
	bisection_search(config_recv[rank][n],new_config,idx_begin_this,idx_end_this,idx,exist);
	new_weight[idx] += weight_recv[rank][n];
      }
    }

    // sum over all weight
    ElemT sum_send = ElemT(0.0);
    for(size_t n=0; n < new_config.size(); n++) {
      sum_send += Conjugate(new_weight[n])*new_weight[n];
    }

    res = ElemT(0.0);
    MPI_Datatype DataT = GetMpiType<ElemT>::MpiT;
    MPI_Allreduce(&sum_send,&res,1,DataT,MPI_SUM,comm);
    
  } // end HamSquare
  
}

#endif // end #ifndef SBD_HCBOSON_OUT_OF_PLACE_FUNC_EXPAND_H

